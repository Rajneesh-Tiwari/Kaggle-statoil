{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put these at the top of every notebook, to get automatic reloading and inline plotting\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit\n",
    "from os.path import join as opj\n",
    "from matplotlib import pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import pylab\n",
    "plt.rcParams['figure.figsize'] = 10, 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Keras.\n",
    "from matplotlib import pyplot\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dense, Dropout, Input, Flatten, Activation\n",
    "from keras.layers import GlobalMaxPooling2D,GlobalAveragePooling2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.merge import Concatenate\n",
    "from keras.models import Model\n",
    "from keras import initializers\n",
    "from keras.optimizers import Adam\n",
    "from keras.optimizers import rmsprop\n",
    "from keras.layers.advanced_activations import LeakyReLU, PReLU\n",
    "from keras.optimizers import SGD\n",
    "from keras.callbacks import ModelCheckpoint, Callback, EarlyStopping,ReduceLROnPlateau\n",
    "\n",
    "from keras.datasets import cifar10\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.applications.xception import Xception\n",
    "from keras.applications.mobilenet import MobileNet\n",
    "from keras.applications.vgg19 import VGG19\n",
    "from keras.layers import Concatenate, Dense, LSTM, Input, concatenate\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from keras.applications.xception import Xception\n",
    "\n",
    "#Data Aug for multi-input\n",
    "from keras.preprocessing.image import ImageDataGenerator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import cv2 # Used to manipulated the images \n",
    "np.random.seed(1337) # The seed I used - pick your own or comment out for a random seed. A constant seed allows for better comparisons though"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = \"F:/Kaggle/Statoil/inputs/\"\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_json(input_path+\"train.json\")\n",
    "target_train=train['is_iceberg']\n",
    "test = pd.read_json(input_path+\"test.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iso(arr):\n",
    "    p = np.reshape(np.array(arr), [75,75]) >(np.mean(np.array(arr))+2*np.std(np.array(arr)))\n",
    "    return p * np.reshape(np.array(arr), [75,75])\n",
    "def size(arr):     \n",
    "    return float(np.sum(arr<-5))/(75*75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### feat engg\n",
    "train['iso1'] = train.iloc[:, 0].apply(iso)\n",
    "train['iso2'] = train.iloc[:, 1].apply(iso)\n",
    "test['iso1'] = test.iloc[:, 0].apply(iso)\n",
    "test['iso2'] = test.iloc[:, 1].apply(iso)\n",
    "\n",
    "# Feature engineering s1 s2 and size.\n",
    "train['s1'] = train.iloc[:,5].apply(size)\n",
    "train['s2'] = train.iloc[:,6].apply(size)\n",
    "test['s1'] = test['iso1'].apply(size)\n",
    "test['s2'] = test['iso2'].apply(size)\n",
    "\n",
    "train['band_1'] = train['band_1'].apply(lambda x: np.array(x).reshape(75, 75))\n",
    "train['band_2'] = train['band_2'].apply(lambda x: np.array(x).reshape(75, 75))\n",
    "test['band_1'] = test['band_1'].apply(lambda x: np.array(x).reshape(75, 75))\n",
    "test['band_2'] = test['band_2'].apply(lambda x: np.array(x).reshape(75, 75))\n",
    "\n",
    "train['inc_angle'] = pd.to_numeric(train['inc_angle'], errors='coerce')\n",
    "test['inc_angle'] = pd.to_numeric(test['inc_angle'], errors='coerce')\n",
    "\n",
    "\n",
    "#####process test set!\n",
    "band_1_test = np.concatenate([im for im in test['band_1']]).reshape(-1, 75, 75)\n",
    "band_2_test = np.concatenate([im for im in test['band_2']]).reshape(-1, 75, 75)\n",
    "full_img_test = np.stack([band_1_test, band_2_test,(band_1_test+band_2_test)/2], axis=1)\n",
    "angle_test=test['inc_angle']\n",
    "size_test=test['s1']\n",
    "test['is_iceberg']=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_train=train['is_iceberg']\n",
    "test['inc_angle']=pd.to_numeric(test['inc_angle'], errors='coerce')\n",
    "train['inc_angle']=pd.to_numeric(train['inc_angle'], errors='coerce')#We have only 133 NAs.\n",
    "train['inc_angle']=train['inc_angle'].fillna(method='pad')\n",
    "X_angle=train['inc_angle']\n",
    "test['inc_angle']=pd.to_numeric(test['inc_angle'], errors='coerce')\n",
    "X_test_angle=test['inc_angle']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### concat angle and size\n",
    "Tr_s1 = train['s1']\n",
    "Tr_s2 = train['s2']\n",
    "Te_s1 = test['s1']\n",
    "Te_s2 = test['s2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scaled_imgs(df):\n",
    "    imgs = []\n",
    "    \n",
    "    for i, row in df.iterrows():\n",
    "        #make 75x75 image\n",
    "        band_1 = np.array(row['band_1']).reshape(75, 75)\n",
    "        band_2 = np.array(row['band_2']).reshape(75, 75)\n",
    "        band_3 = band_1 + band_2 # plus since log(x*y) = log(x) + log(y)\n",
    "        \n",
    "        # Rescale\n",
    "        a = (band_1 - band_1.mean()) / (band_1.max() - band_1.min())\n",
    "        b = (band_2 - band_2.mean()) / (band_2.max() - band_2.min())\n",
    "        c = (band_3 - band_3.mean()) / (band_3.max() - band_3.min())\n",
    "\n",
    "        imgs.append(np.dstack((a, b, c)))\n",
    "\n",
    "    return np.array(imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = get_scaled_imgs(train)\n",
    "X_test = get_scaled_imgs(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"shape of train: {} and test: {}\" .format(X_train.shape,X_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from clr_callback import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### updated to include size and other variables\n",
    "\n",
    "# Define the image transformations here\n",
    "gen = ImageDataGenerator(horizontal_flip = True,\n",
    "                         vertical_flip = True,\n",
    "                         width_shift_range = 0.,\n",
    "                         height_shift_range = 0.,\n",
    "                         channel_shift_range=0,\n",
    "                         zoom_range = 0.2,\n",
    "                         rotation_range = 10)\n",
    "\n",
    "# Here is the function that merges our two generators\n",
    "# We use the exact same generator with the same random seed for both the y and angle arrays\n",
    "\n",
    "def gen_flow_for_two_inputs(X1, X2, s1,s2,y):\n",
    "    genX1 = gen.flow(X1,y,  batch_size=batch_size,seed=55)\n",
    "    genX2 = gen.flow(X1,X2, batch_size=batch_size,seed=55)\n",
    "    genX3 = gen.flow(X1,s1, batch_size=batch_size,seed=55)\n",
    "    genX4 = gen.flow(X1,s2, batch_size=batch_size,seed=55)\n",
    "\n",
    "    while True:\n",
    "            X1i = genX1.next()\n",
    "            X2i = genX2.next()\n",
    "            X3i = genX3.next()\n",
    "            X4i = genX4.next()\n",
    "            \n",
    "            #Assert arrays are equal - this was for peace of mind, but slows down training\n",
    "            #np.testing.assert_array_equal(X1i[0],X2i[0])\n",
    "            yield [X1i[0], (np.vstack((X2i[1],X3i[1],X4i[1])).T)], X1i[1]\n",
    "\n",
    "# Finally create generator\n",
    "# Finally create generator\n",
    "def get_callbacks(filepath, patience=2):\n",
    "    #es = EarlyStopping('val_loss', patience=10, mode=\"min\")\n",
    "    msave = ModelCheckpoint(filepath, save_best_only=True)\n",
    "    #reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2,patience=5, min_lr=0.01)\n",
    "    #clr_triangular = CyclicLR(mode='triangular')\n",
    "    clr_Cyclic = CyclicLR(mode='exp_range', gamma=0.99994)\n",
    "    return [msave,clr_Cyclic]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getXceptionAngleModel():\n",
    "    input_2 = Input(shape=[3,], name=\"angle\")\n",
    "    angle_layer = Dense(1,activation='relu')(input_2)\n",
    "    base_model = Xception(weights='imagenet', include_top=False, pooling='avg')\n",
    "    x = base_model.output\n",
    "    \n",
    "\n",
    "    #x = GlobalMaxPooling2D()(x)\n",
    "    merge_one = concatenate([x, angle_layer])\n",
    "    merge_one = Dense(216, activation='relu', name='fc2')(merge_one)\n",
    "    merge_one = Dropout(0.3)(merge_one)\n",
    "    merge_one = Dense(64, activation='relu', name='fc3')(merge_one)\n",
    "    merge_one = Dropout(0.3)(merge_one)\n",
    "    \n",
    "    predictions = Dense(1, activation='sigmoid')(merge_one)\n",
    "    \n",
    "    model = Model(input=[base_model.input, input_2], output=predictions)\n",
    "    \n",
    "    sgd = SGD(lr=1e-3, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=sgd,\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using K-fold Cross Validation with Data Augmentation.\n",
    "def myAngleCV(X_train, X_angle,Tr_s1,Tr_s2, X_test):\n",
    "    K=2\n",
    "    folds = list(StratifiedKFold(n_splits=K, shuffle=True, random_state=16).split(X_train, target_train))\n",
    "    y_test_pred_log = 0\n",
    "    y_train_pred_log=0\n",
    "    y_valid_pred_log = 0.0*target_train\n",
    "    for j, (train_idx, test_idx) in enumerate(folds):\n",
    "        print('\\n===================FOLD=',j)\n",
    "        X_train_cv = X_train[train_idx]\n",
    "        y_train_cv = target_train[train_idx]\n",
    "        X_holdout = X_train[test_idx]\n",
    "        Y_holdout= target_train[test_idx]\n",
    "        \n",
    "        #Angle +size\n",
    "        X_angle_cv=X_angle[train_idx]\n",
    "        X_angle_hold=X_angle[test_idx]\n",
    "        \n",
    "        #size\n",
    "        X_size1_cv = Tr_s1[train_idx]\n",
    "        X_size2_cv = Tr_s2[train_idx]\n",
    "        X_size1_hold = Tr_s1[test_idx]\n",
    "        X_size2_hold = Tr_s2[test_idx]\n",
    "\n",
    "        #define file path and get callbacks\n",
    "        file_path = \"%s_aug3_model_weights.hdf5\"%j\n",
    "        callbacks = get_callbacks(filepath=file_path, patience=5)\n",
    "        gen_flow = gen_flow_for_two_inputs(X_train_cv, X_angle_cv,X_size1_cv,X_size2_cv, y_train_cv)\n",
    "        galaxyModel= getXceptionAngleModel()\n",
    "        galaxyModel.fit_generator(\n",
    "                gen_flow,\n",
    "                steps_per_epoch=24,\n",
    "                epochs=100,\n",
    "                shuffle=True,\n",
    "                verbose=1,\n",
    "                validation_data=([X_holdout,np.vstack((X_angle_hold,X_size1_hold,X_size2_hold)).T], Y_holdout),\n",
    "                callbacks=callbacks)\n",
    "        \n",
    "\n",
    "        #Getting the Best Model\n",
    "        galaxyModel.load_weights(filepath=file_path)\n",
    "        #Getting Training Score\n",
    "        score = galaxyModel.evaluate([X_train_cv,np.vstack((X_angle_cv,X_size1_cv,X_size2_cv)).T], y_train_cv, verbose=0)\n",
    "        print('Train loss:', score[0])\n",
    "        print('Train accuracy:', score[1])\n",
    "        #Getting Test Score\n",
    "        score = galaxyModel.evaluate([X_holdout,np.vstack((X_angle_hold,X_size1_hold,X_size2_hold)).T], Y_holdout, verbose=0)\n",
    "        print('Test loss:', score[0])\n",
    "        print('Test accuracy:', score[1])\n",
    "\n",
    "        #Getting validation Score.\n",
    "        pred_valid=galaxyModel.predict([X_holdout,np.vstack((X_angle_hold,X_size1_hold,X_size2_hold)).T])\n",
    "        y_valid_pred_log[test_idx] = pred_valid.reshape(pred_valid.shape[0])\n",
    "\n",
    "        #Getting Test Scores\n",
    "        temp_test=galaxyModel.predict([X_test, np.vstack((X_test_angle,Te_s1,Te_s2)).T])\n",
    "        y_test_pred_log+=temp_test.reshape(temp_test.shape[0])\n",
    "\n",
    "        #Getting Train Scores\n",
    "        temp_train=galaxyModel.predict([X_train, np.vstack((X_angle,Tr_s1,Tr_s2)).T])\n",
    "        y_train_pred_log+=temp_train.reshape(temp_train.shape[0])\n",
    "\n",
    "    y_test_pred_log=y_test_pred_log/K\n",
    "    y_train_pred_log=y_train_pred_log/K\n",
    "\n",
    "    print('\\n Train Log Loss Validation= ',log_loss(target_train, y_train_pred_log))\n",
    "    print(' Test Log Loss Validation= ',log_loss(target_train, y_valid_pred_log))\n",
    "    return y_test_pred_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "preds=myAngleCV(X_train, X_angle, Tr_s1,Tr_s2,X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pseudo labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### choose i to maximize best choices for pseudo labels based on prob output\n",
    "i = 0.047\n",
    "idx_pred_1 = (np.where(preds>0.95+i))\n",
    "idx_pred_0 = (np.where(preds<0.05-i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### add pseudolabels data to original train \n",
    "add_test = X_test[idx_pred_1]\n",
    "add_angle = X_test_angle.iloc[idx_pred_1]\n",
    "add_test_s1 = Te_s1.iloc[idx_pred_1]\n",
    "add_test_s2 = Te_s2.iloc[idx_pred_1]\n",
    "\n",
    "add_test1=X_test[idx_pred_0]\n",
    "add_angle1 = X_test_angle.iloc[idx_pred_0]\n",
    "add_test1_s1 = Te_s1.iloc[idx_pred_0]\n",
    "add_test1_s2 = Te_s2.iloc[idx_pred_0]\n",
    "\n",
    "\n",
    "additinal_test = np.concatenate((add_test,add_test1),axis=0)\n",
    "additional_angle = np.concatenate((add_angle,add_angle1),axis=0)\n",
    "additional_s1 = np.concatenate((add_test_s1,add_test1_s1),axis=0)\n",
    "additional_s2 = np.concatenate((add_test_s2,add_test1_s2),axis=0)\n",
    "\n",
    "additional_train = np.concatenate((X_train,additinal_test),axis=0)\n",
    "additional_angle = np.concatenate((X_angle,additional_angle),axis=0)\n",
    "additional_s1_tr = np.concatenate((Tr_s1,additional_s1),axis=0)\n",
    "additional_s2_tr = np.concatenate((Tr_s2,additional_s2),axis=0)\n",
    "\n",
    "\n",
    "Y_temp1 = [1]*add_test.shape[0]\n",
    "Y_temp0 = [0]*add_test1.shape[0]\n",
    "\n",
    "Ynew = np.concatenate((Y_temp1,Y_temp0),axis=0)\n",
    "additionalY = np.concatenate((target_train,Ynew),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "additionalY.shape[0]-target_train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using K-fold Cross Validation with Data Augmentation.\n",
    "def myAngleCV_pseud(additional_train, additional_angle,additional_s1_tr,additional_s2_tr, X_test):\n",
    "    K=5\n",
    "    folds = list(StratifiedKFold(n_splits=K, shuffle=True, random_state=16).split(additional_train, additionalY))\n",
    "    y_test_pred_log = 0\n",
    "    y_train_pred_log=0\n",
    "    y_valid_pred_log = 0.0*additionalY\n",
    "    for j, (train_idx, test_idx) in enumerate(folds):\n",
    "        print('\\n===================FOLD=',j)\n",
    "        X_train_cv = additional_train[train_idx]\n",
    "        y_train_cv = additionalY[train_idx]\n",
    "        X_holdout = additional_train[test_idx]\n",
    "        Y_holdout= additionalY[test_idx]\n",
    "        \n",
    "        #Angle +size\n",
    "        X_angle_cv = additional_angle[train_idx]\n",
    "        X_angle_hold = additional_angle[test_idx]\n",
    "        \n",
    "        #size\n",
    "        X_size1_cv = additional_s1_tr[train_idx]\n",
    "        X_size2_cv = additional_s2_tr[train_idx]\n",
    "        X_size1_hold = additional_s1_tr[test_idx]\n",
    "        X_size2_hold = additional_s2_tr[test_idx]\n",
    "\n",
    "        #define file path and get callbacks\n",
    "        file_path = \"%s_aug4_model_weights.hdf5\"%j\n",
    "        callbacks = get_callbacks(filepath=file_path, patience=5)\n",
    "        gen_flow = gen_flow_for_two_inputs(X_train_cv, X_angle_cv,X_size1_cv,X_size2_cv, y_train_cv)\n",
    "        galaxyModel= getXceptionAngleModel()\n",
    "        galaxyModel.fit_generator(\n",
    "                gen_flow,\n",
    "                steps_per_epoch=24,\n",
    "                epochs=25,\n",
    "                shuffle=True,\n",
    "                verbose=1,\n",
    "                validation_data=([X_holdout,np.vstack((X_angle_hold,X_size1_hold,X_size2_hold)).T], Y_holdout),\n",
    "                callbacks=callbacks)\n",
    "        \n",
    "\n",
    "        #Getting the Best Model\n",
    "        galaxyModel.load_weights(filepath=file_path)\n",
    "        #Getting Training Score\n",
    "        score = galaxyModel.evaluate([X_train_cv,np.vstack((X_angle_cv,X_size1_cv,X_size2_cv)).T], y_train_cv, verbose=0)\n",
    "        print('Train loss:', score[0])\n",
    "        print('Train accuracy:', score[1])\n",
    "        #Getting Test Score\n",
    "        score = galaxyModel.evaluate([X_holdout,np.vstack((X_angle_hold,X_size1_hold,X_size2_hold)).T], Y_holdout, verbose=0)\n",
    "        print('Test loss:', score[0])\n",
    "        print('Test accuracy:', score[1])\n",
    "\n",
    "        #Getting validation Score.\n",
    "        pred_valid=galaxyModel.predict([X_holdout,np.vstack((X_angle_hold,X_size1_hold,X_size2_hold)).T])\n",
    "        y_valid_pred_log[test_idx] = pred_valid.reshape(pred_valid.shape[0])\n",
    "\n",
    "        #Getting Test Scores\n",
    "        temp_test=galaxyModel.predict([X_test, np.vstack((X_test_angle,Te_s1,Te_s2)).T])\n",
    "        y_test_pred_log+=temp_test.reshape(temp_test.shape[0])\n",
    "\n",
    "        #Getting Train Scores\n",
    "        temp_train=galaxyModel.predict([additional_train, np.vstack((additional_angle,additional_s1_tr,additional_s2_tr)).T])\n",
    "        y_train_pred_log+=temp_train.reshape(temp_train.shape[0])\n",
    "\n",
    "    y_test_pred_log=y_test_pred_log/K\n",
    "    y_train_pred_log=y_train_pred_log/K\n",
    "\n",
    "    print('\\n Train Log Loss Validation= ',log_loss(additionalY, y_train_pred_log))\n",
    "    print(' Test Log Loss Validation= ',log_loss(additionalY, y_valid_pred_log))\n",
    "    return y_test_pred_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "predicition_pesudo=myAngleCV_pseud(additional_train, additional_angle,additional_s1_tr,additional_s2_tr, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Submission\n",
    "import datetime as dt\n",
    "\n",
    "submission = pd.DataFrame()\n",
    "submission['id']=test['id']\n",
    "submission['is_iceberg']=predicition_pesudo\n",
    "submission.to_csv('F:/Kaggle/Statoil/submit/sub5cv_xception_pseudolabels_featengg'+dt.datetime.today().strftime(\"%d%m%Y%H%M\")+'.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
